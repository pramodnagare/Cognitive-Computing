{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qERc6CSPSpt0",
    "outputId": "502c253b-f50c-4a98-e1c3-2633a365929b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset : (50000, 3, 32, 32)\n",
      "Shape of testing dataste : (10000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Conv2D, MaxPooling2D, Dense, Flatten,Dropout,Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import SGD \n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "print('Shape of training dataset :',x_train.shape)\n",
    "print('Shape of testing dataste :' ,x_test.shape)\n",
    "\n",
    "#formatting the dataset \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train,10)\n",
    "y_test = np_utils.to_categorical(y_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_job_id = '005'\n",
    "checkpoint_name = 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "filepath = 'saved_models/'\n",
    "epochs_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4050
    },
    "colab_type": "code",
    "id": "QvetEnFuVVcf",
    "outputId": "2101006e-2e97-4995-d67c-99e753c3a8cd"
   },
   "outputs": [],
   "source": [
    "rglrzr = None #regularizers.l2(0.0003)\n",
    "drop_out = 0.3\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    " \n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    " \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.015, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip = False,\n",
    "    #validation_split=0.2\n",
    "    )\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#add some callbacks:\n",
    "callbacks = []\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=filepath+current_job_id+checkpoint_name, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=5)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', min_delta=0.000001, cooldown=0, min_lr=0)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "callbacks.append(checkpoint)\n",
    "callbacks.append(reduce_lr)\n",
    "callbacks.append(early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpiV5bVwIyy4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  9/782 [..............................] - ETA: 18s - loss: 2.7970 - acc: 0.1701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiqidai1002/anaconda3/lib/python3.7/site-packages/mxnet/module/bucketing_module.py:408: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.015625). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 18s 23ms/step - loss: 1.8279 - acc: 0.3375 - val_loss: 1.6176 - val_acc: 0.4262\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 1.4876 - acc: 0.4610 - val_loss: 1.5570 - val_acc: 0.4418\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 1.3507 - acc: 0.5173 - val_loss: 1.3690 - val_acc: 0.5197\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 1.2451 - acc: 0.5567 - val_loss: 1.2947 - val_acc: 0.5517\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 1.1758 - acc: 0.5855 - val_loss: 1.2161 - val_acc: 0.5941\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 1.1048 - acc: 0.6110 - val_loss: 1.1471 - val_acc: 0.6072\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 1.0589 - acc: 0.6285 - val_loss: 0.9889 - val_acc: 0.6610\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.9970 - acc: 0.6500 - val_loss: 1.0529 - val_acc: 0.6495\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.9648 - acc: 0.6628 - val_loss: 0.9078 - val_acc: 0.7008\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.9210 - acc: 0.6804 - val_loss: 0.9168 - val_acc: 0.6834\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.8922 - acc: 0.6908 - val_loss: 0.9286 - val_acc: 0.6865\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.8648 - acc: 0.7005 - val_loss: 0.8365 - val_acc: 0.7211\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.8401 - acc: 0.7107 - val_loss: 0.8427 - val_acc: 0.7251\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.8152 - acc: 0.7178 - val_loss: 0.7161 - val_acc: 0.7581\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.8012 - acc: 0.7275 - val_loss: 0.7084 - val_acc: 0.7634\n",
      "Epoch 16/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.7871 - acc: 0.7317 - val_loss: 0.7320 - val_acc: 0.7518\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.7681 - acc: 0.7347 - val_loss: 0.8756 - val_acc: 0.7131\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.7509 - acc: 0.7422 - val_loss: 0.7551 - val_acc: 0.7560\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.7316 - acc: 0.7485 - val_loss: 0.7536 - val_acc: 0.7555\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.7254 - acc: 0.7529 - val_loss: 0.7728 - val_acc: 0.7532\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.6526 - acc: 0.7747 - val_loss: 0.5981 - val_acc: 0.8009\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.6227 - acc: 0.7843 - val_loss: 0.5990 - val_acc: 0.7995\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.6033 - acc: 0.7932 - val_loss: 0.5948 - val_acc: 0.8013\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.6009 - acc: 0.7914 - val_loss: 0.5841 - val_acc: 0.8058\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5961 - acc: 0.7938 - val_loss: 0.5741 - val_acc: 0.8083\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.5907 - acc: 0.7971 - val_loss: 0.5868 - val_acc: 0.8036\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5867 - acc: 0.7952 - val_loss: 0.5563 - val_acc: 0.8128\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5806 - acc: 0.8000 - val_loss: 0.5754 - val_acc: 0.8025\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.5760 - acc: 0.8015 - val_loss: 0.5607 - val_acc: 0.8106\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5714 - acc: 0.8028 - val_loss: 0.5773 - val_acc: 0.8086\n",
      "Epoch 31/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5684 - acc: 0.8023 - val_loss: 0.5639 - val_acc: 0.8077\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5662 - acc: 0.8018 - val_loss: 0.5619 - val_acc: 0.8124\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5663 - acc: 0.8038 - val_loss: 0.5581 - val_acc: 0.8114\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5617 - acc: 0.8044 - val_loss: 0.5592 - val_acc: 0.8111\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.5597 - acc: 0.8043 - val_loss: 0.5568 - val_acc: 0.8122\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.5582 - acc: 0.8065 - val_loss: 0.5545 - val_acc: 0.8123\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5597 - acc: 0.8063 - val_loss: 0.5550 - val_acc: 0.8122\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5658 - acc: 0.8050 - val_loss: 0.5549 - val_acc: 0.8117\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5494 - acc: 0.8102 - val_loss: 0.5595 - val_acc: 0.8116\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.5521 - acc: 0.8089 - val_loss: 0.5567 - val_acc: 0.8114\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5582 - acc: 0.8086 - val_loss: 0.5571 - val_acc: 0.8112\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5540 - acc: 0.8074 - val_loss: 0.5548 - val_acc: 0.8129\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5526 - acc: 0.8068 - val_loss: 0.5549 - val_acc: 0.8128\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5565 - acc: 0.8056 - val_loss: 0.5554 - val_acc: 0.8123\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5554 - acc: 0.8066 - val_loss: 0.5548 - val_acc: 0.8124\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5538 - acc: 0.8077 - val_loss: 0.5594 - val_acc: 0.8105\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5533 - acc: 0.8082 - val_loss: 0.5553 - val_acc: 0.8122\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.5558 - acc: 0.8071 - val_loss: 0.5558 - val_acc: 0.8123\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.5550 - acc: 0.8074 - val_loss: 0.5555 - val_acc: 0.8129\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5540 - acc: 0.8068 - val_loss: 0.5547 - val_acc: 0.8122\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5558 - acc: 0.8083 - val_loss: 0.5559 - val_acc: 0.8110\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5538 - acc: 0.8067 - val_loss: 0.5561 - val_acc: 0.8119\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.5512 - acc: 0.8117 - val_loss: 0.5570 - val_acc: 0.8114\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5509 - acc: 0.8063 - val_loss: 0.5556 - val_acc: 0.8121\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.5599 - acc: 0.8072 - val_loss: 0.5566 - val_acc: 0.8119\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.5560 - acc: 0.8076 - val_loss: 0.5555 - val_acc: 0.8121\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-06d29c8ffd86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                               callbacks = callbacks)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_y' is not defined"
     ]
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=64), \n",
    "                              epochs=epochs_n,\n",
    "                              validation_data=(x_test,y_test),\n",
    "                              callbacks = callbacks)\n",
    "\n",
    "score = model.evaluate(x=x_test,y=test_y)\n",
    "print('score:' + score)\n",
    "\n",
    "model.save(filepath + current_job_id)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "research.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
