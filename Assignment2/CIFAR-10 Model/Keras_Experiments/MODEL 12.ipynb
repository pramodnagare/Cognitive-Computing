{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXPERIMENT+12.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qhvo1s_KzQpb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1530
        },
        "outputId": "daa38fe3-58dc-468f-a8db-2a45a7e5c63b"
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import  Conv2D, MaxPooling2D, Dense, Flatten,Dropout,Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import SGD \n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
        "\n",
        "print('Shape of training dataset :',x_train.shape)\n",
        "print('Shape of testing dataste :' ,x_test.shape)\n",
        "\n",
        "#formatting the dataset \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,10)\n",
        "y_test = np_utils.to_categorical(y_test,10)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3,3),input_shape=[32,32,3],kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(32,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l2(0.0005),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(256,(3,3),kernel_regularizer=regularizers.l2(0.0005),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "SGD = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = False)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) / 32, epochs=50,\n",
        "                    validation_data = (x_test,y_test))\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset : (50000, 32, 32, 3)\n",
            "Shape of testing dataste : (10000, 32, 32, 3)\n",
            "Epoch 1/50\n",
            "1563/1562 [==============================] - 91s 58ms/step - loss: 2.5378 - acc: 0.3300 - val_loss: 3.7736 - val_acc: 0.1080\n",
            "Epoch 2/50\n",
            "1563/1562 [==============================] - 85s 54ms/step - loss: 2.0627 - acc: 0.4547 - val_loss: 3.4612 - val_acc: 0.1970\n",
            "Epoch 3/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.8882 - acc: 0.5173 - val_loss: 3.5834 - val_acc: 0.1700\n",
            "Epoch 4/50\n",
            "1563/1562 [==============================] - 87s 56ms/step - loss: 1.7728 - acc: 0.5589 - val_loss: 3.6244 - val_acc: 0.1700\n",
            "Epoch 5/50\n",
            "1563/1562 [==============================] - 88s 57ms/step - loss: 1.6684 - acc: 0.5926 - val_loss: 3.8745 - val_acc: 0.1882\n",
            "Epoch 6/50\n",
            "1563/1562 [==============================] - 89s 57ms/step - loss: 1.5895 - acc: 0.6250 - val_loss: 3.5829 - val_acc: 0.2310\n",
            "Epoch 7/50\n",
            "1563/1562 [==============================] - 88s 57ms/step - loss: 1.5228 - acc: 0.6469 - val_loss: 3.4473 - val_acc: 0.2565\n",
            "Epoch 8/50\n",
            "1563/1562 [==============================] - 87s 56ms/step - loss: 1.4625 - acc: 0.6617 - val_loss: 3.7383 - val_acc: 0.1885\n",
            "Epoch 9/50\n",
            "1563/1562 [==============================] - 91s 58ms/step - loss: 1.4098 - acc: 0.6806 - val_loss: 3.6859 - val_acc: 0.2295\n",
            "Epoch 10/50\n",
            "1563/1562 [==============================] - 90s 57ms/step - loss: 1.3671 - acc: 0.6938 - val_loss: 3.1408 - val_acc: 0.3010\n",
            "Epoch 11/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.3272 - acc: 0.7049 - val_loss: 3.2754 - val_acc: 0.3136\n",
            "Epoch 12/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.2962 - acc: 0.7111 - val_loss: 3.6076 - val_acc: 0.2267\n",
            "Epoch 13/50\n",
            "1563/1562 [==============================] - 89s 57ms/step - loss: 1.2598 - acc: 0.7232 - val_loss: 3.2386 - val_acc: 0.2775\n",
            "Epoch 14/50\n",
            "1563/1562 [==============================] - 88s 57ms/step - loss: 1.2348 - acc: 0.7312 - val_loss: 3.6601 - val_acc: 0.2592\n",
            "Epoch 15/50\n",
            "1563/1562 [==============================] - 87s 56ms/step - loss: 1.2116 - acc: 0.7380 - val_loss: 3.1298 - val_acc: 0.2891\n",
            "Epoch 16/50\n",
            "1563/1562 [==============================] - 90s 57ms/step - loss: 1.1813 - acc: 0.7443 - val_loss: 3.4915 - val_acc: 0.2779\n",
            "Epoch 17/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.1512 - acc: 0.7520 - val_loss: 3.3329 - val_acc: 0.2928\n",
            "Epoch 18/50\n",
            "1563/1562 [==============================] - 89s 57ms/step - loss: 1.1387 - acc: 0.7566 - val_loss: 3.6319 - val_acc: 0.2301\n",
            "Epoch 19/50\n",
            "1563/1562 [==============================] - 87s 56ms/step - loss: 1.1155 - acc: 0.7604 - val_loss: 2.8579 - val_acc: 0.3105\n",
            "Epoch 20/50\n",
            "1563/1562 [==============================] - 91s 58ms/step - loss: 1.0980 - acc: 0.7649 - val_loss: 3.1851 - val_acc: 0.2683\n",
            "Epoch 21/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.0729 - acc: 0.7716 - val_loss: 2.9970 - val_acc: 0.2904\n",
            "Epoch 22/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.0560 - acc: 0.7753 - val_loss: 3.0417 - val_acc: 0.3182\n",
            "Epoch 23/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 1.0426 - acc: 0.7785 - val_loss: 2.5564 - val_acc: 0.3704\n",
            "Epoch 24/50\n",
            "1563/1562 [==============================] - 92s 59ms/step - loss: 1.0237 - acc: 0.7823 - val_loss: 2.9243 - val_acc: 0.3088\n",
            "Epoch 25/50\n",
            "1563/1562 [==============================] - 91s 58ms/step - loss: 1.0082 - acc: 0.7865 - val_loss: 2.6066 - val_acc: 0.3754\n",
            "Epoch 26/50\n",
            "1563/1562 [==============================] - 87s 56ms/step - loss: 0.9962 - acc: 0.7883 - val_loss: 2.8154 - val_acc: 0.3292\n",
            "Epoch 27/50\n",
            "1563/1562 [==============================] - 87s 56ms/step - loss: 0.9817 - acc: 0.7934 - val_loss: 2.5635 - val_acc: 0.3747\n",
            "Epoch 28/50\n",
            "1563/1562 [==============================] - 88s 57ms/step - loss: 0.9680 - acc: 0.7938 - val_loss: 2.7186 - val_acc: 0.3649\n",
            "Epoch 29/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 0.9564 - acc: 0.7991 - val_loss: 2.8086 - val_acc: 0.3523\n",
            "Epoch 30/50\n",
            "1563/1562 [==============================] - 91s 58ms/step - loss: 0.9429 - acc: 0.7990 - val_loss: 2.8832 - val_acc: 0.3362\n",
            "Epoch 31/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 0.9259 - acc: 0.8045 - val_loss: 2.6970 - val_acc: 0.3516\n",
            "Epoch 32/50\n",
            "1563/1562 [==============================] - 88s 57ms/step - loss: 0.9156 - acc: 0.8065 - val_loss: 2.6762 - val_acc: 0.3547\n",
            "Epoch 33/50\n",
            "1563/1562 [==============================] - 92s 59ms/step - loss: 0.9094 - acc: 0.8070 - val_loss: 2.7717 - val_acc: 0.3354\n",
            "Epoch 34/50\n",
            "1563/1562 [==============================] - 90s 58ms/step - loss: 0.8929 - acc: 0.8128 - val_loss: 2.6020 - val_acc: 0.3684\n",
            "Epoch 35/50\n",
            "1563/1562 [==============================] - 89s 57ms/step - loss: 0.8889 - acc: 0.8095 - val_loss: 2.4204 - val_acc: 0.4100\n",
            "Epoch 36/50\n",
            "1563/1562 [==============================] - 88s 56ms/step - loss: 0.8768 - acc: 0.8119 - val_loss: 2.4957 - val_acc: 0.3799\n",
            "Epoch 37/50\n",
            "1563/1562 [==============================] - 91s 58ms/step - loss: 0.8649 - acc: 0.8171 - val_loss: 2.5123 - val_acc: 0.3838\n",
            "Epoch 38/50\n",
            "1563/1562 [==============================] - 92s 59ms/step - loss: 0.8592 - acc: 0.8182 - val_loss: 2.0992 - val_acc: 0.4441\n",
            "Epoch 39/50\n",
            "1563/1562 [==============================] - 88s 57ms/step - loss: 0.8473 - acc: 0.8206 - val_loss: 2.4669 - val_acc: 0.3945\n",
            "Epoch 40/50\n",
            "1563/1562 [==============================] - 92s 59ms/step - loss: 0.8385 - acc: 0.8214 - val_loss: 2.7329 - val_acc: 0.3668\n",
            "Epoch 41/50\n",
            "1563/1562 [==============================] - 90s 58ms/step - loss: 0.8320 - acc: 0.8237 - val_loss: 2.2610 - val_acc: 0.4453\n",
            "Epoch 42/50\n",
            "1563/1562 [==============================] - 89s 57ms/step - loss: 0.8249 - acc: 0.8237 - val_loss: 2.2373 - val_acc: 0.4429\n",
            "Epoch 43/50\n",
            " 752/1562 [=============>................] - ETA: 43s - loss: 0.8189 - acc: 0.8249Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u_mlowrQzUa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}