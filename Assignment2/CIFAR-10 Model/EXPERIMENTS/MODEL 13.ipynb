{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXPERIMENT+13.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rUpH32gn7Lfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1983
        },
        "outputId": "c1235755-05a3-41f8-e9f6-b8c4b5cf9cfe"
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import  Conv2D, MaxPooling2D, Dense, Flatten,Dropout,Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import SGD \n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
        "\n",
        "print('Shape of training dataset :',x_train.shape)\n",
        "print('Shape of testing dataste :' ,x_test.shape)\n",
        "\n",
        "#formatting the dataset \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,10)\n",
        "y_test = np_utils.to_categorical(y_test,10)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3,3),input_shape=[32,32,3],kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(32,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l2(0.0005),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(256,(3,3),kernel_regularizer=regularizers.l2(0.0005),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "#SGD = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = False)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) / 32, epochs=50,\n",
        "                    validation_data = (x_test,y_test))\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset : (50000, 32, 32, 3)\n",
            "Shape of testing dataste : (10000, 32, 32, 3)\n",
            "Epoch 1/50\n",
            "1563/1562 [==============================] - 112s 72ms/step - loss: 3.7055 - acc: 0.1668 - val_loss: 5.8632 - val_acc: 0.1205\n",
            "Epoch 2/50\n",
            "1563/1562 [==============================] - 107s 69ms/step - loss: 3.3943 - acc: 0.2046 - val_loss: 2.9847 - val_acc: 0.1561\n",
            "Epoch 3/50\n",
            "1563/1562 [==============================] - 110s 70ms/step - loss: 2.6660 - acc: 0.2468 - val_loss: 2.6688 - val_acc: 0.1509\n",
            "Epoch 4/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 2.1142 - acc: 0.3203 - val_loss: 3.3312 - val_acc: 0.1297\n",
            "Epoch 5/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.8928 - acc: 0.3853 - val_loss: 3.4605 - val_acc: 0.1070\n",
            "Epoch 6/50\n",
            "1563/1562 [==============================] - 109s 70ms/step - loss: 1.7216 - acc: 0.4545 - val_loss: 3.1293 - val_acc: 0.1188\n",
            "Epoch 7/50\n",
            "1563/1562 [==============================] - 110s 70ms/step - loss: 1.5978 - acc: 0.5103 - val_loss: 3.3326 - val_acc: 0.1447\n",
            "Epoch 8/50\n",
            "1563/1562 [==============================] - 110s 71ms/step - loss: 1.4856 - acc: 0.5579 - val_loss: 3.5831 - val_acc: 0.1188\n",
            "Epoch 9/50\n",
            "1563/1562 [==============================] - 109s 70ms/step - loss: 1.3948 - acc: 0.6003 - val_loss: 3.4452 - val_acc: 0.1642\n",
            "Epoch 10/50\n",
            "1563/1562 [==============================] - 108s 69ms/step - loss: 1.3210 - acc: 0.6335 - val_loss: 3.1159 - val_acc: 0.2520\n",
            "Epoch 11/50\n",
            "1563/1562 [==============================] - 108s 69ms/step - loss: 1.2870 - acc: 0.6510 - val_loss: 2.3243 - val_acc: 0.3389\n",
            "Epoch 12/50\n",
            "1563/1562 [==============================] - 110s 71ms/step - loss: 1.2524 - acc: 0.6605 - val_loss: 2.5078 - val_acc: 0.3303\n",
            "Epoch 13/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.2237 - acc: 0.6758 - val_loss: 2.9059 - val_acc: 0.2522\n",
            "Epoch 14/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.1959 - acc: 0.6867 - val_loss: 2.8952 - val_acc: 0.2918\n",
            "Epoch 15/50\n",
            "1563/1562 [==============================] - 110s 71ms/step - loss: 1.1710 - acc: 0.6985 - val_loss: 3.2452 - val_acc: 0.2600\n",
            "Epoch 16/50\n",
            "1563/1562 [==============================] - 107s 69ms/step - loss: 1.1639 - acc: 0.6984 - val_loss: 2.6411 - val_acc: 0.2716\n",
            "Epoch 17/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.1449 - acc: 0.7082 - val_loss: 2.6260 - val_acc: 0.3302\n",
            "Epoch 18/50\n",
            "1563/1562 [==============================] - 110s 70ms/step - loss: 1.1230 - acc: 0.7147 - val_loss: 2.6076 - val_acc: 0.3108\n",
            "Epoch 19/50\n",
            "1563/1562 [==============================] - 109s 70ms/step - loss: 1.1139 - acc: 0.7220 - val_loss: 2.2198 - val_acc: 0.3495\n",
            "Epoch 20/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.0986 - acc: 0.7264 - val_loss: 2.4006 - val_acc: 0.3325\n",
            "Epoch 21/50\n",
            "1563/1562 [==============================] - 112s 72ms/step - loss: 1.0921 - acc: 0.7277 - val_loss: 2.0500 - val_acc: 0.4214\n",
            "Epoch 22/50\n",
            "1563/1562 [==============================] - 108s 69ms/step - loss: 1.0796 - acc: 0.7329 - val_loss: 2.4223 - val_acc: 0.3433\n",
            "Epoch 23/50\n",
            "1563/1562 [==============================] - 109s 70ms/step - loss: 1.0758 - acc: 0.7327 - val_loss: 2.3117 - val_acc: 0.3631\n",
            "Epoch 24/50\n",
            "1563/1562 [==============================] - 110s 71ms/step - loss: 1.0718 - acc: 0.7368 - val_loss: 2.3306 - val_acc: 0.3546\n",
            "Epoch 25/50\n",
            "1563/1562 [==============================] - 112s 72ms/step - loss: 1.0598 - acc: 0.7433 - val_loss: 2.0288 - val_acc: 0.4152\n",
            "Epoch 26/50\n",
            "1563/1562 [==============================] - 109s 70ms/step - loss: 1.0525 - acc: 0.7448 - val_loss: 2.2691 - val_acc: 0.3343\n",
            "Epoch 27/50\n",
            "1563/1562 [==============================] - 109s 70ms/step - loss: 1.0473 - acc: 0.7444 - val_loss: 2.0134 - val_acc: 0.4186\n",
            "Epoch 28/50\n",
            "1563/1562 [==============================] - 107s 69ms/step - loss: 1.0491 - acc: 0.7487 - val_loss: 2.2493 - val_acc: 0.3605\n",
            "Epoch 29/50\n",
            "1563/1562 [==============================] - 110s 70ms/step - loss: 1.0407 - acc: 0.7509 - val_loss: 2.3403 - val_acc: 0.3680\n",
            "Epoch 30/50\n",
            "1563/1562 [==============================] - 109s 69ms/step - loss: 1.0374 - acc: 0.7501 - val_loss: 2.2858 - val_acc: 0.3672\n",
            "Epoch 31/50\n",
            "1563/1562 [==============================] - 107s 69ms/step - loss: 1.0313 - acc: 0.7564 - val_loss: 2.1257 - val_acc: 0.3995\n",
            "Epoch 32/50\n",
            "1563/1562 [==============================] - 113s 72ms/step - loss: 1.0290 - acc: 0.7542 - val_loss: 2.3788 - val_acc: 0.3308\n",
            "Epoch 33/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.0294 - acc: 0.7542 - val_loss: 2.2400 - val_acc: 0.3794\n",
            "Epoch 34/50\n",
            "1563/1562 [==============================] - 113s 72ms/step - loss: 1.0204 - acc: 0.7548 - val_loss: 2.1918 - val_acc: 0.4019\n",
            "Epoch 35/50\n",
            "1563/1562 [==============================] - 114s 73ms/step - loss: 1.0274 - acc: 0.7549 - val_loss: 2.3623 - val_acc: 0.3243\n",
            "Epoch 36/50\n",
            "1563/1562 [==============================] - 111s 71ms/step - loss: 1.0148 - acc: 0.7608 - val_loss: 2.4764 - val_acc: 0.3427\n",
            "Epoch 37/50\n",
            "1563/1562 [==============================] - 132s 84ms/step - loss: 1.0115 - acc: 0.7620 - val_loss: 2.3368 - val_acc: 0.3571\n",
            "Epoch 38/50\n",
            "1563/1562 [==============================] - 276s 177ms/step - loss: 1.0171 - acc: 0.7591 - val_loss: 2.4164 - val_acc: 0.3690\n",
            "Epoch 39/50\n",
            "1563/1562 [==============================] - 219s 140ms/step - loss: 1.0133 - acc: 0.7594 - val_loss: 2.3299 - val_acc: 0.3650\n",
            "Epoch 40/50\n",
            "1563/1562 [==============================] - 322s 206ms/step - loss: 1.0106 - acc: 0.7631 - val_loss: 2.6782 - val_acc: 0.2776\n",
            "Epoch 41/50\n",
            "1563/1562 [==============================] - 399s 255ms/step - loss: 1.0094 - acc: 0.7630 - val_loss: 2.1684 - val_acc: 0.3989\n",
            "Epoch 42/50\n",
            "1563/1562 [==============================] - 394s 252ms/step - loss: 1.0077 - acc: 0.7644 - val_loss: 2.4967 - val_acc: 0.3255\n",
            "Epoch 43/50\n",
            "1563/1562 [==============================] - 387s 248ms/step - loss: 1.0079 - acc: 0.7637 - val_loss: 2.6137 - val_acc: 0.3224\n",
            "Epoch 44/50\n",
            "1563/1562 [==============================] - 404s 259ms/step - loss: 0.9944 - acc: 0.7699 - val_loss: 2.4237 - val_acc: 0.3548\n",
            "Epoch 45/50\n",
            "1563/1562 [==============================] - 407s 260ms/step - loss: 0.9958 - acc: 0.7660 - val_loss: 2.4831 - val_acc: 0.3480\n",
            "Epoch 46/50\n",
            "1563/1562 [==============================] - 405s 259ms/step - loss: 0.9978 - acc: 0.7665 - val_loss: 2.1100 - val_acc: 0.4104\n",
            "Epoch 47/50\n",
            "1563/1562 [==============================] - 400s 256ms/step - loss: 0.9982 - acc: 0.7661 - val_loss: 2.4630 - val_acc: 0.2886\n",
            "Epoch 48/50\n",
            "1563/1562 [==============================] - 302s 193ms/step - loss: 0.9941 - acc: 0.7689 - val_loss: 2.3021 - val_acc: 0.3994\n",
            "Epoch 49/50\n",
            "1563/1562 [==============================] - 300s 192ms/step - loss: 0.9908 - acc: 0.7695 - val_loss: 2.4961 - val_acc: 0.3276\n",
            "Epoch 50/50\n",
            "1563/1562 [==============================] - 305s 195ms/step - loss: 0.9967 - acc: 0.7676 - val_loss: 2.3783 - val_acc: 0.3553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ab00fea17589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m                     validation_data = (x_test,y_test))\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Wc51ZXbA7R4L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}