{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXPERIMENT+5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0-L8qSmjLg2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1982
        },
        "outputId": "42078ce1-590d-4d1d-b4c8-6629410a2d4e"
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import  Conv2D, MaxPooling2D, Dense, Flatten,Dropout,Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import SGD \n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
        "\n",
        "print('Shape of training dataset :',x_train.shape)\n",
        "print('Shape of testing dataste :' ,x_test.shape)\n",
        "\n",
        "#formatting the dataset \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,10)\n",
        "y_test = np_utils.to_categorical(y_test,10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,(3,3),activation = 'relu',input_shape=[32,32,3],kernel_regularizer=regularizers.l1(0.01)))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(32,(3,3),activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation = 'relu',kernel_regularizer =regularizers.l1(0.01)))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l1(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),activation='relu',kernel_regularizer =regularizers.l1(0.01)))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l1(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = False)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) / 32, epochs=50,\n",
        "                    validation_data = (x_test,y_test))\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset : (50000, 32, 32, 3)\n",
            "Shape of testing dataste : (10000, 32, 32, 3)\n",
            "Epoch 1/50\n",
            "1563/1562 [==============================] - 68s 43ms/step - loss: 4.4286 - acc: 0.1032 - val_loss: 2.7942 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6560 - acc: 0.1007 - val_loss: 2.7048 - val_acc: 0.1000\n",
            "Epoch 3/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6563 - acc: 0.0979 - val_loss: 2.6800 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6573 - acc: 0.1002 - val_loss: 2.6855 - val_acc: 0.1000\n",
            "Epoch 5/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6593 - acc: 0.0995 - val_loss: 2.7347 - val_acc: 0.1000\n",
            "Epoch 6/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6582 - acc: 0.1002 - val_loss: 2.6810 - val_acc: 0.1000\n",
            "Epoch 7/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6602 - acc: 0.1008 - val_loss: 2.7016 - val_acc: 0.1000\n",
            "Epoch 8/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6587 - acc: 0.1004 - val_loss: 2.8124 - val_acc: 0.1000\n",
            "Epoch 9/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6595 - acc: 0.1013 - val_loss: 2.7070 - val_acc: 0.1000\n",
            "Epoch 10/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6592 - acc: 0.0992 - val_loss: 2.7532 - val_acc: 0.1000\n",
            "Epoch 11/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6582 - acc: 0.0998 - val_loss: 2.7097 - val_acc: 0.1000\n",
            "Epoch 12/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6604 - acc: 0.0974 - val_loss: 2.7626 - val_acc: 0.1000\n",
            "Epoch 13/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6619 - acc: 0.0993 - val_loss: 2.8001 - val_acc: 0.1000\n",
            "Epoch 14/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6570 - acc: 0.0972 - val_loss: 2.7294 - val_acc: 0.1000\n",
            "Epoch 15/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6556 - acc: 0.1003 - val_loss: 2.7211 - val_acc: 0.1000\n",
            "Epoch 16/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6574 - acc: 0.1007 - val_loss: 2.7227 - val_acc: 0.1000\n",
            "Epoch 17/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6587 - acc: 0.0975 - val_loss: 2.7416 - val_acc: 0.1000\n",
            "Epoch 18/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6590 - acc: 0.1004 - val_loss: 2.7406 - val_acc: 0.1000\n",
            "Epoch 19/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6584 - acc: 0.0989 - val_loss: 2.7218 - val_acc: 0.1000\n",
            "Epoch 20/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6601 - acc: 0.0987 - val_loss: 2.6799 - val_acc: 0.1000\n",
            "Epoch 21/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6585 - acc: 0.0999 - val_loss: 2.9846 - val_acc: 0.1000\n",
            "Epoch 22/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6568 - acc: 0.1025 - val_loss: 2.7967 - val_acc: 0.1000\n",
            "Epoch 23/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6576 - acc: 0.1002 - val_loss: 2.7992 - val_acc: 0.1000\n",
            "Epoch 24/50\n",
            "1563/1562 [==============================] - 64s 41ms/step - loss: 2.6566 - acc: 0.0983 - val_loss: 2.7830 - val_acc: 0.1000\n",
            "Epoch 25/50\n",
            "1563/1562 [==============================] - 65s 41ms/step - loss: 2.6574 - acc: 0.0993 - val_loss: 2.6970 - val_acc: 0.1000\n",
            "Epoch 26/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6570 - acc: 0.0990 - val_loss: 2.7359 - val_acc: 0.1000\n",
            "Epoch 27/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6564 - acc: 0.1027 - val_loss: 2.7748 - val_acc: 0.1000\n",
            "Epoch 28/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6548 - acc: 0.1003 - val_loss: 2.7560 - val_acc: 0.1000\n",
            "Epoch 29/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6565 - acc: 0.0996 - val_loss: 2.7337 - val_acc: 0.1000\n",
            "Epoch 30/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6559 - acc: 0.1006 - val_loss: 2.7906 - val_acc: 0.1000\n",
            "Epoch 31/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6578 - acc: 0.1020 - val_loss: 2.7377 - val_acc: 0.1000\n",
            "Epoch 32/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6575 - acc: 0.0995 - val_loss: 2.7798 - val_acc: 0.1000\n",
            "Epoch 33/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6574 - acc: 0.1029 - val_loss: 2.7650 - val_acc: 0.1000\n",
            "Epoch 34/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6572 - acc: 0.1014 - val_loss: 2.7883 - val_acc: 0.1000\n",
            "Epoch 35/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6577 - acc: 0.1020 - val_loss: 2.8959 - val_acc: 0.1000\n",
            "Epoch 36/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6585 - acc: 0.0986 - val_loss: 2.8324 - val_acc: 0.1000\n",
            "Epoch 37/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6590 - acc: 0.0979 - val_loss: 2.7203 - val_acc: 0.1000\n",
            "Epoch 38/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6584 - acc: 0.1013 - val_loss: 2.8531 - val_acc: 0.1000\n",
            "Epoch 39/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6571 - acc: 0.1029 - val_loss: 2.8032 - val_acc: 0.1000\n",
            "Epoch 40/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6565 - acc: 0.1022 - val_loss: 2.7921 - val_acc: 0.1000\n",
            "Epoch 41/50\n",
            "1563/1562 [==============================] - 65s 42ms/step - loss: 2.6580 - acc: 0.0991 - val_loss: 2.7715 - val_acc: 0.1000\n",
            "Epoch 42/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6560 - acc: 0.0998 - val_loss: 2.7299 - val_acc: 0.1000\n",
            "Epoch 43/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6557 - acc: 0.0994 - val_loss: 2.8644 - val_acc: 0.1000\n",
            "Epoch 44/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6561 - acc: 0.1009 - val_loss: 2.7457 - val_acc: 0.1000\n",
            "Epoch 45/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6561 - acc: 0.1009 - val_loss: 2.7006 - val_acc: 0.1000\n",
            "Epoch 46/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6558 - acc: 0.1009 - val_loss: 2.8083 - val_acc: 0.1000\n",
            "Epoch 47/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6563 - acc: 0.0990 - val_loss: 2.7134 - val_acc: 0.1000\n",
            "Epoch 48/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6557 - acc: 0.1005 - val_loss: 2.7004 - val_acc: 0.1000\n",
            "Epoch 49/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6555 - acc: 0.1015 - val_loss: 2.9284 - val_acc: 0.1000\n",
            "Epoch 50/50\n",
            "1563/1562 [==============================] - 66s 42ms/step - loss: 2.6562 - acc: 0.1006 - val_loss: 2.8092 - val_acc: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8f88e4b07ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m                     validation_data = (x_test,y_test))\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4yYHDteLL__z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}