{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXPERIMENT+14.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lIEFvX1sKkvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2663
        },
        "outputId": "621e0e99-6c3a-4adf-b2df-7487bc98566c"
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import  Conv2D, MaxPooling2D, Dense, Flatten,Dropout,Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "from keras import regularizers\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import SGD \n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
        "\n",
        "print('Shape of training dataset :',x_train.shape)\n",
        "print('Shape of testing dataste :' ,x_test.shape)\n",
        "\n",
        "#formatting the dataset \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,10)\n",
        "y_test = np_utils.to_categorical(y_test,10)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32,(3,3),input_shape=[32,32,3],kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(32,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(64,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(128,(3,3),kernel_regularizer=regularizers.l2(0.0005),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(256,(3,3),kernel_regularizer=regularizers.l2(0.0005),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(512,(3,3),kernel_regularizer=regularizers.l2(0.0003),padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "SGD = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip = False)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) / 64, epochs=70,\n",
        "                    validation_data = (x_test,y_test))\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset : (50000, 32, 32, 3)\n",
            "Shape of testing dataste : (10000, 32, 32, 3)\n",
            "Epoch 1/70\n",
            "782/781 [==============================] - 115s 147ms/step - loss: 2.9821 - acc: 0.2749 - val_loss: 3.7568 - val_acc: 0.1231\n",
            "Epoch 2/70\n",
            "782/781 [==============================] - 133s 170ms/step - loss: 2.4483 - acc: 0.3714 - val_loss: 3.8234 - val_acc: 0.1161\n",
            "Epoch 3/70\n",
            "782/781 [==============================] - 129s 165ms/step - loss: 2.2776 - acc: 0.4289 - val_loss: 3.5230 - val_acc: 0.1583\n",
            "Epoch 4/70\n",
            "782/781 [==============================] - 96s 122ms/step - loss: 2.1841 - acc: 0.4617 - val_loss: 4.0473 - val_acc: 0.1009\n",
            "Epoch 5/70\n",
            "782/781 [==============================] - 94s 120ms/step - loss: 2.0894 - acc: 0.4988 - val_loss: 3.3650 - val_acc: 0.1900\n",
            "Epoch 6/70\n",
            "782/781 [==============================] - 125s 159ms/step - loss: 2.0194 - acc: 0.5213 - val_loss: 3.6424 - val_acc: 0.1912\n",
            "Epoch 7/70\n",
            "782/781 [==============================] - 175s 224ms/step - loss: 1.9654 - acc: 0.5421 - val_loss: 3.7082 - val_acc: 0.1799\n",
            "Epoch 8/70\n",
            "782/781 [==============================] - 173s 221ms/step - loss: 1.9089 - acc: 0.5624 - val_loss: 3.7622 - val_acc: 0.1822\n",
            "Epoch 9/70\n",
            "782/781 [==============================] - 174s 223ms/step - loss: 1.8459 - acc: 0.5798 - val_loss: 3.4538 - val_acc: 0.2273\n",
            "Epoch 10/70\n",
            "782/781 [==============================] - 172s 219ms/step - loss: 1.8135 - acc: 0.5939 - val_loss: 3.4776 - val_acc: 0.2199\n",
            "Epoch 11/70\n",
            "782/781 [==============================] - 175s 223ms/step - loss: 1.7619 - acc: 0.6097 - val_loss: 3.4459 - val_acc: 0.2265\n",
            "Epoch 12/70\n",
            "782/781 [==============================] - 175s 224ms/step - loss: 1.7230 - acc: 0.6231 - val_loss: 3.2814 - val_acc: 0.2164\n",
            "Epoch 13/70\n",
            "782/781 [==============================] - 174s 222ms/step - loss: 1.6901 - acc: 0.6366 - val_loss: 3.4386 - val_acc: 0.1824\n",
            "Epoch 14/70\n",
            "782/781 [==============================] - 171s 218ms/step - loss: 1.6533 - acc: 0.6453 - val_loss: 3.1429 - val_acc: 0.2941\n",
            "Epoch 15/70\n",
            "782/781 [==============================] - 178s 228ms/step - loss: 1.6156 - acc: 0.6605 - val_loss: 3.5948 - val_acc: 0.1998\n",
            "Epoch 16/70\n",
            "782/781 [==============================] - 180s 230ms/step - loss: 1.6017 - acc: 0.6633 - val_loss: 3.3478 - val_acc: 0.2636\n",
            "Epoch 17/70\n",
            "782/781 [==============================] - 178s 227ms/step - loss: 1.5583 - acc: 0.6776 - val_loss: 3.3707 - val_acc: 0.2672\n",
            "Epoch 18/70\n",
            "782/781 [==============================] - 178s 228ms/step - loss: 1.5534 - acc: 0.6774 - val_loss: 3.3375 - val_acc: 0.2947\n",
            "Epoch 19/70\n",
            "782/781 [==============================] - 178s 227ms/step - loss: 1.5116 - acc: 0.6917 - val_loss: 3.1230 - val_acc: 0.2711\n",
            "Epoch 20/70\n",
            "782/781 [==============================] - 178s 228ms/step - loss: 1.5039 - acc: 0.6901 - val_loss: 3.2184 - val_acc: 0.2479\n",
            "Epoch 21/70\n",
            "782/781 [==============================] - 179s 229ms/step - loss: 1.4773 - acc: 0.6994 - val_loss: 3.0502 - val_acc: 0.3066\n",
            "Epoch 22/70\n",
            "782/781 [==============================] - 179s 230ms/step - loss: 1.4471 - acc: 0.7078 - val_loss: 3.1595 - val_acc: 0.2528\n",
            "Epoch 23/70\n",
            "782/781 [==============================] - 166s 212ms/step - loss: 1.4435 - acc: 0.7086 - val_loss: 3.3886 - val_acc: 0.2227\n",
            "Epoch 24/70\n",
            "782/781 [==============================] - 140s 180ms/step - loss: 1.4264 - acc: 0.7110 - val_loss: 3.3610 - val_acc: 0.2568\n",
            "Epoch 25/70\n",
            "782/781 [==============================] - 141s 180ms/step - loss: 1.3884 - acc: 0.7251 - val_loss: 3.1280 - val_acc: 0.2945\n",
            "Epoch 26/70\n",
            "782/781 [==============================] - 137s 175ms/step - loss: 1.3909 - acc: 0.7226 - val_loss: 2.9622 - val_acc: 0.3027\n",
            "Epoch 27/70\n",
            "782/781 [==============================] - 138s 177ms/step - loss: 1.3680 - acc: 0.7277 - val_loss: 3.0023 - val_acc: 0.2877\n",
            "Epoch 28/70\n",
            "782/781 [==============================] - 139s 177ms/step - loss: 1.3691 - acc: 0.7279 - val_loss: 3.1437 - val_acc: 0.2547\n",
            "Epoch 29/70\n",
            "782/781 [==============================] - 140s 178ms/step - loss: 1.3325 - acc: 0.7367 - val_loss: 3.1742 - val_acc: 0.2659\n",
            "Epoch 30/70\n",
            "782/781 [==============================] - 111s 142ms/step - loss: 1.3286 - acc: 0.7398 - val_loss: 2.9429 - val_acc: 0.3419\n",
            "Epoch 31/70\n",
            "782/781 [==============================] - 97s 124ms/step - loss: 1.3145 - acc: 0.7422 - val_loss: 3.2019 - val_acc: 0.3237\n",
            "Epoch 32/70\n",
            "782/781 [==============================] - 94s 120ms/step - loss: 1.2908 - acc: 0.7465 - val_loss: 3.1983 - val_acc: 0.2968\n",
            "Epoch 33/70\n",
            "782/781 [==============================] - 95s 121ms/step - loss: 1.2875 - acc: 0.7481 - val_loss: 3.3646 - val_acc: 0.2590\n",
            "Epoch 34/70\n",
            "782/781 [==============================] - 97s 124ms/step - loss: 1.2718 - acc: 0.7524 - val_loss: 3.2357 - val_acc: 0.3208\n",
            "Epoch 35/70\n",
            "782/781 [==============================] - 84s 107ms/step - loss: 1.2608 - acc: 0.7533 - val_loss: 3.1050 - val_acc: 0.2972\n",
            "Epoch 36/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.2578 - acc: 0.7550 - val_loss: 2.9690 - val_acc: 0.3042\n",
            "Epoch 37/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.2332 - acc: 0.7614 - val_loss: 3.2286 - val_acc: 0.3200\n",
            "Epoch 38/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.2277 - acc: 0.7588 - val_loss: 3.2327 - val_acc: 0.3257\n",
            "Epoch 39/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.2133 - acc: 0.7649 - val_loss: 2.9772 - val_acc: 0.3123\n",
            "Epoch 40/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1973 - acc: 0.7685 - val_loss: 2.8471 - val_acc: 0.3303\n",
            "Epoch 41/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1880 - acc: 0.7704 - val_loss: 3.0877 - val_acc: 0.2929\n",
            "Epoch 42/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1775 - acc: 0.7706 - val_loss: 3.1386 - val_acc: 0.2989\n",
            "Epoch 43/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1687 - acc: 0.7727 - val_loss: 2.8276 - val_acc: 0.3543\n",
            "Epoch 44/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1661 - acc: 0.7731 - val_loss: 3.1711 - val_acc: 0.3086\n",
            "Epoch 45/70\n",
            "782/781 [==============================] - 54s 69ms/step - loss: 1.1578 - acc: 0.7764 - val_loss: 3.0001 - val_acc: 0.3001\n",
            "Epoch 46/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1515 - acc: 0.7740 - val_loss: 2.4072 - val_acc: 0.4238\n",
            "Epoch 47/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1281 - acc: 0.7824 - val_loss: 2.7095 - val_acc: 0.3636\n",
            "Epoch 48/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1325 - acc: 0.7799 - val_loss: 2.8480 - val_acc: 0.3443\n",
            "Epoch 49/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.1198 - acc: 0.7840 - val_loss: 3.1760 - val_acc: 0.3049\n",
            "Epoch 50/70\n",
            "782/781 [==============================] - 54s 68ms/step - loss: 1.1134 - acc: 0.7860 - val_loss: 3.1373 - val_acc: 0.3530\n",
            "Epoch 51/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0988 - acc: 0.7871 - val_loss: 2.5893 - val_acc: 0.4057\n",
            "Epoch 52/70\n",
            "782/781 [==============================] - 54s 69ms/step - loss: 1.0935 - acc: 0.7880 - val_loss: 2.7559 - val_acc: 0.3528\n",
            "Epoch 53/70\n",
            "782/781 [==============================] - 54s 69ms/step - loss: 1.0807 - acc: 0.7942 - val_loss: 2.5880 - val_acc: 0.4014\n",
            "Epoch 54/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0787 - acc: 0.7912 - val_loss: 2.9919 - val_acc: 0.3214\n",
            "Epoch 55/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0649 - acc: 0.7968 - val_loss: 2.7532 - val_acc: 0.3702\n",
            "Epoch 56/70\n",
            "782/781 [==============================] - 54s 68ms/step - loss: 1.0629 - acc: 0.7962 - val_loss: 2.7031 - val_acc: 0.3372\n",
            "Epoch 57/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0519 - acc: 0.7992 - val_loss: 2.4320 - val_acc: 0.4247\n",
            "Epoch 58/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0478 - acc: 0.8010 - val_loss: 2.9364 - val_acc: 0.3658\n",
            "Epoch 59/70\n",
            "782/781 [==============================] - 54s 68ms/step - loss: 1.0389 - acc: 0.8008 - val_loss: 3.2087 - val_acc: 0.2967\n",
            "Epoch 60/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0320 - acc: 0.7984 - val_loss: 3.1147 - val_acc: 0.2905\n",
            "Epoch 61/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0189 - acc: 0.8065 - val_loss: 2.6975 - val_acc: 0.3334\n",
            "Epoch 62/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0219 - acc: 0.8054 - val_loss: 2.8241 - val_acc: 0.3469\n",
            "Epoch 63/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 1.0114 - acc: 0.8076 - val_loss: 2.6484 - val_acc: 0.3648\n",
            "Epoch 64/70\n",
            "782/781 [==============================] - 54s 68ms/step - loss: 1.0023 - acc: 0.8074 - val_loss: 2.4930 - val_acc: 0.3992\n",
            "Epoch 65/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 0.9921 - acc: 0.8087 - val_loss: 2.6330 - val_acc: 0.3978\n",
            "Epoch 66/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 0.9923 - acc: 0.8097 - val_loss: 2.4243 - val_acc: 0.4146\n",
            "Epoch 67/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 0.9743 - acc: 0.8132 - val_loss: 2.9817 - val_acc: 0.3430\n",
            "Epoch 68/70\n",
            "782/781 [==============================] - 54s 69ms/step - loss: 0.9837 - acc: 0.8096 - val_loss: 2.4343 - val_acc: 0.4037\n",
            "Epoch 69/70\n",
            "782/781 [==============================] - 53s 68ms/step - loss: 0.9657 - acc: 0.8135 - val_loss: 2.5928 - val_acc: 0.3883\n",
            "Epoch 70/70\n",
            "782/781 [==============================] - 54s 69ms/step - loss: 0.9652 - acc: 0.8154 - val_loss: 2.3985 - val_acc: 0.3960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-454972713e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m                     validation_data = (x_test,y_test))\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}